{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4.2 - Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Exercise 4.2, we will introduce a new natural language processing task to figure out which meaning of a word is present in a given context. This task is known as Word Sense Disambiguation. Knowing the correct meaning, or _sense,_ of a word is useful for many downstream NLP tasks like translation or question answering. Often, sense-tagged text is used as an intermediate representation for later NLP tasks too, like reasoning about sentences.\n",
    "\n",
    "How do we humans disambiguate? We often make a choice for the correct sense considering other information from context, which includes not only the sentence with the word, but the larger document, the author, and our world knowledge. In this exercise, you'll try two WSD approaches to solving the task: \n",
    "  * A supervised learning approach that uses the classic Logistic Regression that you have used in previous assignments\n",
    "  * A knowledge-based approach that doesn't rely on machine learning and instead relies on information from an external knowledge base  \n",
    "\n",
    "The second approach will use the Lesk algorithm, which despite being proposed in 1980s is still considered a reasonable baseline for testing. In this exercise's setup, both approaches will look only at the containing-sentence as context.\n",
    "\n",
    "Let's see how it will perform on our dataset, which will be derived from SemCor, a subsection of the Brown Corpus (American English from the 1960s) where all content words are tagged with their WordNet senses. \n",
    "\n",
    "This exercise has the following learning goals:\n",
    "* Familiarize you with knowledge-based approaches to NLP\n",
    "* Practice using supervised machine learning (this time for a semantic task)\n",
    "* Learn how to work with WordNet and SemCor data structures\n",
    "* Familiarize yourself with NLTK methods, which is another common NLP library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import semcor\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus.reader.wordnet import Lemma\n",
    "from nltk.tree import Tree\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the random seed to 655 for the convenience of replication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In Exercise 4.2, we will use the SemCor corpus as our dataset. SemCor is a semantically annotated English corpus, and it can be directly imported via NLTK. Words in SemCor are annotated using WordNet synsets. To load annotated sentences in SemCor, we can simply call `semcor.tagged_sents()`. Besides sense annotations, SemCor words are also annotated with part-of-speech. Since we will focus on word sense disambugation in this exercise, we only need to pull out sense annotations, calling `semcor.tagged_sents(tag='sem')`.\n",
    "\n",
    "The returned sentences from `semcor.tagged_sents(tag='sem')` are in the format of lists of trees. Each sentence is represented as a list of trees (Tree is a special data structure in NLTK). To make them more model-friendly, we will generate two lists for each sentence - a list of words in the sentence and a list of corresponding senses for each word in the sentence. Clearly, they should have the same length.\n",
    "\n",
    "The ultimate goal of the data preparation is to make (features, label) pairs from the SemCor corpus as our train/test data. Each (ambiguous) word contributes to a datapoint. The label is the sense of the (ambiguous) word, and the features consist of the context words of the (ambiguous) word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "semcor_tagged_sents = semcor.tagged_sents(tag='sem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one example of a tagged sentence to have a sense of \"a list of trees\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The'],\n",
       " Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]),\n",
       " Tree(Lemma('state.v.01.say'), ['said']),\n",
       " Tree(Lemma('friday.n.01.Friday'), ['Friday']),\n",
       " ['an'],\n",
       " Tree(Lemma('probe.n.01.investigation'), ['investigation']),\n",
       " ['of'],\n",
       " Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']),\n",
       " [\"'s\"],\n",
       " Tree(Lemma('late.s.03.recent'), ['recent']),\n",
       " Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']),\n",
       " Tree(Lemma('produce.v.04.produce'), ['produced']),\n",
       " ['``'],\n",
       " ['no'],\n",
       " Tree(Lemma('evidence.n.01.evidence'), ['evidence']),\n",
       " [\"''\"],\n",
       " ['that'],\n",
       " ['any'],\n",
       " Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']),\n",
       " Tree(Lemma('happen.v.01.take_place'), ['took', 'place']),\n",
       " ['.']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semcor_tagged_sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above might look intimidating. Don't worry about that! Below is some code to generate the two lists we want for each sentence. You can check the code inside the second for-loop to have some idea how to interact with and extract the information from a tree. The following code works on generating two lists for each sentence and saving them as a tuple (list A, list B) into an overall list `list_of_lemmas_and_senses`.\n",
    "\n",
    "* List A - a list of words in the sentence\n",
    "* List B - a list of annotated senses for each word in the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.1: Create a dataset for WSD from SemCor\n",
    "SemCor by itself doesn't have a dataset, so we'll create one to use for the purposes of this exercise. Here, we'll loop through all the SemCore tagged sentences and extract a list of senses and lemmas (and their parts of speech) in each sentence.\n",
    "\n",
    "Complete the following code by filling in the TODO section\n",
    "\n",
    "Note: this is the longest section of the exercise and can take ~3 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "166b0755b5028c08a863ab23a7cdb3c5",
     "grade": false,
     "grade_id": "cell-d26d92768923b18d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37176/37176 [01:07<00:00, 546.82it/s] \n"
     ]
    }
   ],
   "source": [
    "list_of_lemmas_and_senses = []\n",
    "\n",
    "for sent in tqdm(semcor_tagged_sents):\n",
    "    # List of (lemma, pos) tuples\n",
    "    lemmas = []\n",
    "    senses = []\n",
    "    \n",
    "    for constituent in sent:\n",
    "        if isinstance(constituent, Tree):\n",
    "            if isinstance(constituent.label(), Lemma):\n",
    "                # Most sense-tagged lemmas have a name like \"analyze.v.01.study\"\n",
    "                # The first three parts are the synset by sense number and the fourth\n",
    "                # is the lemma form of the current word, which is what we want.\n",
    "                #\n",
    "                # Sometimes it only has three parts, in which case we use the first\n",
    "                label_components = constituent.label().synset().name().split('.')\n",
    "                if len(label_components) != 4:               \n",
    "                    curr_lemma = label_components[0]\n",
    "                else:\n",
    "                    curr_lemma = label_components[3]\n",
    "                curr_pos = label_components[1]\n",
    "\n",
    "                corresponding_sense = constituent.label().synset()\n",
    "                \n",
    "                # TODO: append the current word and its parts of speech (as a tuple) \n",
    "                # to 'lemmas' and append the corresponding sense to 'senses'\n",
    "                # YOUR CODE HERE\n",
    "                lemmas.append((curr_lemma, curr_pos))\n",
    "                senses.append(corresponding_sense)\n",
    "            else:\n",
    "                lemmas += [(w, 'other') for w in constituent.leaves()]\n",
    "                senses += [None] * len(constituent.leaves())\n",
    "        else:\n",
    "            lemmas += [(w, 'other') for w in constituent]\n",
    "            senses += [None] * len(constituent)\n",
    "    \n",
    "    list_of_lemmas_and_senses.append((lemmas, senses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have `list_of_lemmas_and_senses`, we are a huge step closer to the ultimate goal of data preparation. The next step is to generate the labeled data for each word. For each datapoint/word, the features are the context words of the current word, and the label is the correct sense of the current word. We also keep the current word along with its sense and features so that we can check word frequencies later, so as to include only the most common 100 words. There are several reasons to only include 100 most frequent words:\n",
    "* 1) The frequency distribution of words generally follows the Zipf's Law, so there is a lot of words with low frequency, which means there is not enough train/test data for those words.\n",
    "* 2) Including all the words will make the train/test data super large, which might exceed the running memory limit of the server, though most of words don't even have enough train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37176/37176 [00:00<00:00, 40139.46it/s]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for lemmas_pos, senses in tqdm(list_of_lemmas_and_senses):\n",
    "    lemmas = [lp[0] for lp in lemmas_pos]\n",
    "    for i, (lemma, pos) in enumerate(lemmas_pos):\n",
    "        # Check whether this lemma had a sense, and if not, skip it\n",
    "        if senses[i] is None:\n",
    "            continue\n",
    "            \n",
    "        context = ' '.join(lemmas[:i] + lemmas[(i + 1):])\n",
    "        correct_sense = senses[i].name()\n",
    "        # Out of convenience, we'll join the lemma and part of speech together into\n",
    "        # a single string, which lets us keep the contexts distinct for words with\n",
    "        # more than one part of speech\n",
    "        data.append([lemma + '.' + pos, context, correct_sense])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the data into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.DataFrame(data=data, columns=['lemma.pos', 'context', 'sense'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.2: Print the dataset size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6086dda69266d0c6f3e04cca856ed669",
     "grade": true,
     "grade_id": "cell-a516721c28b3501a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "224716\n"
     ]
    }
   ],
   "source": [
    "print(len(data_df)) # should be 224716\n",
    "# hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a sanity check for the first 10 datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma.pos</th>\n",
       "      <th>context</th>\n",
       "      <th>sense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group.n</td>\n",
       "      <td>The state friday an probe of atlanta 's late p...</td>\n",
       "      <td>group.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>state.v</td>\n",
       "      <td>The group friday an probe of atlanta 's late p...</td>\n",
       "      <td>state.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friday.n</td>\n",
       "      <td>The group state an probe of atlanta 's late pr...</td>\n",
       "      <td>friday.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>probe.n</td>\n",
       "      <td>The group state friday an of atlanta 's late p...</td>\n",
       "      <td>probe.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>atlanta.n</td>\n",
       "      <td>The group state friday an probe of 's late pri...</td>\n",
       "      <td>atlanta.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>late.s</td>\n",
       "      <td>The group state friday an probe of atlanta 's ...</td>\n",
       "      <td>late.s.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>primary.n</td>\n",
       "      <td>The group state friday an probe of atlanta 's ...</td>\n",
       "      <td>primary.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>produce.v</td>\n",
       "      <td>The group state friday an probe of atlanta 's ...</td>\n",
       "      <td>produce.v.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>evidence.n</td>\n",
       "      <td>The group state friday an probe of atlanta 's ...</td>\n",
       "      <td>evidence.n.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>abnormality.n</td>\n",
       "      <td>The group state friday an probe of atlanta 's ...</td>\n",
       "      <td>abnormality.n.04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma.pos                                            context  \\\n",
       "0        group.n  The state friday an probe of atlanta 's late p...   \n",
       "1        state.v  The group friday an probe of atlanta 's late p...   \n",
       "2       friday.n  The group state an probe of atlanta 's late pr...   \n",
       "3        probe.n  The group state friday an of atlanta 's late p...   \n",
       "4      atlanta.n  The group state friday an probe of 's late pri...   \n",
       "5         late.s  The group state friday an probe of atlanta 's ...   \n",
       "6      primary.n  The group state friday an probe of atlanta 's ...   \n",
       "7      produce.v  The group state friday an probe of atlanta 's ...   \n",
       "8     evidence.n  The group state friday an probe of atlanta 's ...   \n",
       "9  abnormality.n  The group state friday an probe of atlanta 's ...   \n",
       "\n",
       "              sense  \n",
       "0        group.n.01  \n",
       "1        state.v.01  \n",
       "2       friday.n.01  \n",
       "3        probe.n.01  \n",
       "4      atlanta.n.01  \n",
       "5         late.s.03  \n",
       "6      primary.n.01  \n",
       "7      produce.v.04  \n",
       "8     evidence.n.01  \n",
       "9  abnormality.n.04  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.3: Print the most common 100 annotated lemmas in the SemCor corpus.\n",
    "\n",
    "There are a lot of lemmas in our dataset&mdash;in fact WordNet contains hundreds of thousand words. To keep things simple for this exercise, we will only use the most common 100 of them. Let's print them out to see how they look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b3130c600286fadc52f492b1d672e0be",
     "grade": true,
     "grade_id": "cell-6201adab27e87118",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('be.v', 14586),\n",
       " ('person.n', 6768),\n",
       " ('state.v', 1945),\n",
       " ('make.v', 1758),\n",
       " ('not.r', 1703),\n",
       " ('have.v', 1701),\n",
       " ('group.n', 1338),\n",
       " ('location.n', 989),\n",
       " ('look.v', 974),\n",
       " ('know.v', 883),\n",
       " ('exist.v', 814),\n",
       " ('see.v', 744),\n",
       " ('use.v', 720),\n",
       " ('become.v', 687),\n",
       " ('think.v', 616),\n",
       " ('two.s', 571),\n",
       " ('give.v', 564),\n",
       " ('then.r', 542),\n",
       " ('besides.r', 541),\n",
       " ('merely.r', 514),\n",
       " ('one.s', 513),\n",
       " ('time.n', 499),\n",
       " ('man.n', 492),\n",
       " ('ask.v', 470),\n",
       " ('come.v', 462),\n",
       " ('so.r', 460),\n",
       " ('get_down.v', 454),\n",
       " ('find.v', 444),\n",
       " ('other.a', 442),\n",
       " ('even.r', 436),\n",
       " ('year.n', 425),\n",
       " ('get.v', 423),\n",
       " ('leave.v', 413),\n",
       " ('put.v', 410),\n",
       " ('desire.v', 395),\n",
       " ('travel.v', 393),\n",
       " ('try.v', 393),\n",
       " ('more.r', 387),\n",
       " ('take.v', 372),\n",
       " ('happen.v', 353),\n",
       " ('supply.v', 352),\n",
       " ('necessitate.v', 344),\n",
       " ('continue.v', 337),\n",
       " ('keep.v', 335),\n",
       " ('day.n', 334),\n",
       " ('bring.v', 331),\n",
       " ('let.v', 325),\n",
       " ('determine.v', 320),\n",
       " ('feel.v', 318),\n",
       " ('understand.v', 314),\n",
       " ('many.a', 309),\n",
       " ('arrive.v', 306),\n",
       " ('learn.v', 296),\n",
       " ('small.a', 295),\n",
       " ('state.n', 294),\n",
       " ('equal.v', 294),\n",
       " ('three.s', 292),\n",
       " ('meet.v', 292),\n",
       " ('include.v', 284),\n",
       " ('reach.v', 280),\n",
       " ('new.a', 276),\n",
       " ('help.v', 276),\n",
       " ('entirely.r', 274),\n",
       " ('tell.v', 272),\n",
       " ('receive.v', 271),\n",
       " ('experience.v', 271),\n",
       " ('however.r', 270),\n",
       " ('remember.v', 269),\n",
       " ('now.r', 266),\n",
       " ('child.n', 265),\n",
       " ('about.r', 263),\n",
       " ('expect.v', 260),\n",
       " ('show.v', 259),\n",
       " ('hold.v', 258),\n",
       " ('stay.v', 258),\n",
       " ('here.r', 255),\n",
       " ('people.n', 255),\n",
       " ('very.r', 251),\n",
       " ('write.v', 248),\n",
       " ('own.s', 246),\n",
       " ('manner.n', 244),\n",
       " ('detect.v', 243),\n",
       " ('hear.v', 242),\n",
       " ('thing.n', 241),\n",
       " ('talk.v', 239),\n",
       " ('stand.v', 239),\n",
       " ('kind.n', 238),\n",
       " ('prove.v', 238),\n",
       " ('constitute.v', 237),\n",
       " ('most.r', 237),\n",
       " ('perform.v', 234),\n",
       " ('again.r', 229),\n",
       " ('enter.v', 228),\n",
       " ('large.a', 227),\n",
       " ('never.r', 223),\n",
       " ('establish.v', 222),\n",
       " ('allege.v', 221),\n",
       " ('part.n', 221),\n",
       " ('appear.v', 217),\n",
       " ('still.r', 217)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_counts = Counter(data_df['lemma.pos'])\n",
    "top_100_lemmas = lemma_counts.most_common(100)\n",
    "#hidden tests are within this cell\n",
    "top_100_lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Two Approaches to Resolving Word Sense Disambiguation\n",
    "\n",
    "The data we need has been ready. That's great! Now we can move forward to solve the word sense disambuguation task. Here we will introduce two approaches -\n",
    "* Supervised learning approach\n",
    "* Knowledge-based approach\n",
    "\n",
    "Supervised learning approach has the training process. After it recognizes the pattern in the training data, it will be able to apply what it learns to make predictions on the test data. Knowledge-based approach doesn't have the training process. Instead, it has its own knowledge base. In the scenario here, the knowledge base is the definition of each sense of each word provided by WordNet. Knowledge-based approach makes it prediction with the help of its knowledge base. Specifically, the supervised learning approach we will use is **Logistic Regression**, and the knowledge-based approach we will use is the **Lesk** algorithm. We will use both of these two approaches to predict word senses and then compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Learning Approach - Logistic Regression\n",
    "\n",
    "For the supervised learning approach, we come back to our old friend - logistic regression. Tfidf vectorizer will be used to transform the textual data into a model-friendly matrix of Tfidf features. We split datapoints for each word into train and test by the ratio of 80/20, and train separate classifiers for each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.4: Create a Tfidf vectorizer that we will use to convert context words into model-friendly features.\n",
    "\n",
    "Create a Tfidf vectorizer that will remove stopwords and any words whose document frequency is less than 500. \n",
    "\n",
    "We're keeping this classifier simple for testing purposes, but you are welcome to explore the effects of different hyperparameter settings. What happens if you allow for more features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "b1c796032551f167e94e959e64508a20",
     "grade": false,
     "grade_id": "cell-9ecbd5c431ea6308",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(min_df=500, stop_words='english')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO - create a Tfidf vectorizer\n",
    "# YOUR CODE HERE\n",
    "vectorizer = TfidfVectorizer(min_df=500, stop_words='english')\n",
    "\n",
    "\n",
    "vectorizer.fit(data_df['context'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.5: Complete function `generate_train_test_data_for_each_word`\n",
    "\n",
    "`generate_train_test_data_for_each_word` takes a word and the `data_df` data frame as the input. First, it extracts datapoints associated with the word (i.e. the label of the datapoint is a sense associated with the word). Then, it splits the data using `train_test_split` with a 80%-train-20%-test ratio and the preset `RANDOM_SEED` as the random_state. Last, it returns X_train, X_test, y_train, and y_test produced by `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "554d0cfb71a0e449a75613a0d616764b",
     "grade": false,
     "grade_id": "cell-c47c84324c6ccc5c",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "  lemma_pos: the current lemma and its POS as a string (e.g., \"become.v\")\n",
    "  data_df: data frame for all datapoints [lemma, context, sense]\n",
    "return:\n",
    "  X_train: list of contexts, each as a string\n",
    "  X_test: list of contexts, each as a string\n",
    "  y_train: list of senses, each as a string\n",
    "  y_test: list of senses, each as a string\n",
    "if there is only a single annotated sense for the current lemma, skip the lemma\n",
    "'''\n",
    "def generate_train_test_data_for_each_lemma(lemma_pos, data_df):\n",
    "    lemma_df = data_df[data_df['lemma.pos'] == lemma_pos]\n",
    "\n",
    "    # if there is only one class, return None's to indicate no need to train a classifier\n",
    "    if len(lemma_df['sense'].unique()) < 2:\n",
    "        return [None] * 4\n",
    "    \n",
    "\n",
    "    X = lemma_df['context'].tolist()\n",
    "    y = lemma_df['sense'].tolist()\n",
    "    # TODO - split the datapoints with a 80%-train-20%-test ratio and the preset `RANDOM_SEED` as the random_state\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_SEED)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.6: Complete function `train_classifiers_for_each_lemma`\n",
    "\n",
    "`train_classifiers_for_each_lemma` takes the most common 100 words, the `data_df` dataframe, and the Tfidf vectorizer that has been done fitting as parameters. It iterates through the words given and trains a classifier for each word. We save the trained classifiers in a dictionary `clf_dict` - with word as key and its classifier as value. We also save the test data for each word in a dictionary `test_data_dict` - with word as key and a tuple (`X_test`, `y_test`) as value. The reason why we save them is that we will use them as the test data for the knowledge-based approach. We make sure the test data is the same for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3b0405b610b5e4d06e0703e1a4c7c5be",
     "grade": false,
     "grade_id": "cell-b8937e14083d1226",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "  lemmas_with_pos: a  list of the current lemmas and their POS \n",
    "  data_df: data frame for all datapoints [word, context, sense]\n",
    "  vectorizer: Tfidf vectorizer that has been fit on all contexts from the Semcor corpus\n",
    "return:\n",
    "  clf_dict: key-value as {word: its specific trained classifier}\n",
    "  test_data_dict: key-value as {word: (test_features, test_labels)}\n",
    "'''\n",
    "def train_classifiers_for_each_lemma(lemmas_with_pos, data_df, vectorizer):\n",
    "    clf_dict = {}\n",
    "    test_data_dict = {}\n",
    "\n",
    "    for lemma_pos in tqdm(lemmas_with_pos):\n",
    "        X_train, X_test, y_train, y_test = generate_train_test_data_for_each_lemma(lemma_pos, data_df)\n",
    "\n",
    "        # X_train is None indicates that there is only one class for the current word.\n",
    "        # It's meaningless to train such a classifier, so we will use 'continue' to skip.\n",
    "        if X_train is None:\n",
    "            continue\n",
    "\n",
    "        clf = LogisticRegression(multi_class='auto', solver='lbfgs', max_iter=1000, random_state=RANDOM_SEED)\n",
    "        \n",
    "        # TODO - fit the classifier (don't forget to use vectorizer to transform X_train)\n",
    "        # YOUR CODE HERE\n",
    "        X_train = vectorizer.transform(X_train)\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        \n",
    "        # TODO - save the 'clf' into 'clf_dict'\n",
    "        # YOUR CODE HERE\n",
    "        clf_dict[lemma_pos] = clf \n",
    "        \n",
    "        # TODO - save the tuple (X_test, y_test) into 'test_data_dict'\n",
    "        # YOUR CODE HERE\n",
    "        test_data_dict[lemma_pos] = (X_test, y_test)\n",
    "\n",
    "    return clf_dict, test_data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin training 100 classifiers for the most common 100 annotated words, with each word having one classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:05<00:00, 16.84it/s]\n"
     ]
    }
   ],
   "source": [
    "clf_dict, test_data_dict = train_classifiers_for_each_lemma([x[0] for x in top_100_lemmas], data_df, vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the performance of our trained classifiers on their corresponding test data. Here we use `clf.score()` as the metric, which returns accuracy. For each word, we save the classification result into `clf_results`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.7: Complete the following code for evaluating supervised learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "3053dd147b676ce238bd0506c21d564f",
     "grade": false,
     "grade_id": "cell-ae8249e340604e6b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:00<00:00, 327.44it/s]\n"
     ]
    }
   ],
   "source": [
    "clf_results = {}\n",
    "\n",
    "for lemma_pos, clf in tqdm(clf_dict.items()):\n",
    "    \n",
    "    # TODO - retrieve the data from 'test_data_dict' for X_test and y_test\n",
    "    # YOUR CODE HERE\n",
    "    X_test, y_test = test_data_dict[lemma_pos][0], test_data_dict[lemma_pos][1]\n",
    "    \n",
    "    clf_results[lemma_pos] = clf.score(vectorizer.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'be.v': 0.6686086360520904,\n",
       " 'make.v': 0.5596590909090909,\n",
       " 'have.v': 0.7243401759530792,\n",
       " 'group.n': 0.9888059701492538,\n",
       " 'look.v': 0.8,\n",
       " 'know.v': 0.6045197740112994,\n",
       " 'exist.v': 0.9631901840490797,\n",
       " 'see.v': 0.6442953020134228,\n",
       " 'use.v': 0.9583333333333334,\n",
       " 'become.v': 0.6376811594202898,\n",
       " 'think.v': 0.5241935483870968,\n",
       " 'give.v': 0.336283185840708,\n",
       " 'then.r': 0.6330275229357798,\n",
       " 'besides.r': 0.981651376146789,\n",
       " 'one.s': 0.9902912621359223,\n",
       " 'time.n': 0.42,\n",
       " 'man.n': 0.898989898989899,\n",
       " 'ask.v': 0.5957446808510638,\n",
       " 'come.v': 0.5591397849462365,\n",
       " 'so.r': 0.6413043478260869,\n",
       " 'get_down.v': 0.989010989010989,\n",
       " 'find.v': 0.5168539325842697,\n",
       " 'even.r': 0.8977272727272727,\n",
       " 'year.n': 0.9882352941176471,\n",
       " 'get.v': 0.5764705882352941,\n",
       " 'leave.v': 0.4457831325301205,\n",
       " 'put.v': 0.8414634146341463,\n",
       " 'desire.v': 1.0,\n",
       " 'travel.v': 0.9620253164556962,\n",
       " 'try.v': 1.0,\n",
       " 'more.r': 0.9230769230769231,\n",
       " 'take.v': 0.41333333333333333,\n",
       " 'happen.v': 0.7746478873239436,\n",
       " 'necessitate.v': 0.9855072463768116,\n",
       " 'continue.v': 0.6764705882352942,\n",
       " 'keep.v': 0.8805970149253731,\n",
       " 'day.n': 0.5522388059701493,\n",
       " 'bring.v': 0.5074626865671642,\n",
       " 'let.v': 0.9692307692307692,\n",
       " 'determine.v': 0.53125,\n",
       " 'feel.v': 0.453125,\n",
       " 'understand.v': 0.6190476190476191,\n",
       " 'arrive.v': 0.967741935483871,\n",
       " 'learn.v': 0.65,\n",
       " 'state.n': 0.6101694915254238,\n",
       " 'equal.v': 0.9661016949152542,\n",
       " 'meet.v': 0.3050847457627119,\n",
       " 'include.v': 0.8070175438596491,\n",
       " 'reach.v': 0.39285714285714285,\n",
       " 'help.v': 0.8392857142857143,\n",
       " 'tell.v': 0.6727272727272727,\n",
       " 'receive.v': 0.6545454545454545,\n",
       " 'experience.v': 0.9454545454545454,\n",
       " 'however.r': 0.9074074074074074,\n",
       " 'remember.v': 0.5740740740740741,\n",
       " 'now.r': 0.8518518518518519,\n",
       " 'child.n': 0.8490566037735849,\n",
       " 'about.r': 0.8301886792452831,\n",
       " 'expect.v': 0.8076923076923077,\n",
       " 'show.v': 0.6923076923076923,\n",
       " 'hold.v': 0.46153846153846156,\n",
       " 'stay.v': 0.6538461538461539,\n",
       " 'here.r': 0.6274509803921569,\n",
       " 'people.n': 0.9803921568627451,\n",
       " 'very.r': 0.9607843137254902,\n",
       " 'write.v': 0.62,\n",
       " 'manner.n': 0.9183673469387755,\n",
       " 'hear.v': 0.9387755102040817,\n",
       " 'thing.n': 0.32653061224489793,\n",
       " 'talk.v': 0.6875,\n",
       " 'stand.v': 0.5208333333333334,\n",
       " 'prove.v': 0.5416666666666666,\n",
       " 'most.r': 0.6041666666666666,\n",
       " 'perform.v': 0.8297872340425532,\n",
       " 'enter.v': 0.8478260869565217,\n",
       " 'establish.v': 0.4888888888888889,\n",
       " 'part.n': 0.5555555555555556,\n",
       " 'appear.v': 0.45454545454545453,\n",
       " 'still.r': 1.0}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Based Approach - Lesk\n",
    "\n",
    "The Lesk algorithm will be used as the knowledge-based approach in this assignment. The idea behind Lesk is  straight-forward: Given a word in a sentence, for the WordNet Synset definition of each sense of that word, we compute the number of words in common between the context and the word's WordNet Synset definition. The sense, whose definition has the most number of words in common with the context of the given word, will be considered the correct sense for the given word.\n",
    "\n",
    "For example:\n",
    "(this is an artificial example only for easy demonstration. The definitions in the knowledge base are actually different)\n",
    "\n",
    "> The word \"apple\" has two senses: 1) a kind of fruit 2) a kind of tree 2) a tech company.<br>\n",
    ">\n",
    "> Given a sentence: \"I buy tech products in an apple store\".<br>\n",
    ">\n",
    "> What does the \"apple\" mean here? A kind of fruit, a kind of tree, or a tech company? The answer given by Lesk will be 3) a tech company. The reason is that the context \\[\"I\", \"buy\", \"tech\", \"\"products\", \"in\", \"an\", \"store\"\\] has **one** word in common with definition 3), which is the greatest number in all definitions.\n",
    "\n",
    "In this assignment, we will tweak the original Lesk algorithm to expand the definition of context. Since WordNet contains other types of information like example uses of each sense, it's common to include these examples as \"extended glosses\" when comparing with the sentence.\n",
    "\n",
    "What do we do if the context has no overlap with any of the senses' information? In such cases, we can take advantage of the structure of WordNet itself! WordNet was designed to order senses by frequency so that the most common sense is the first sense (though this is not _always_ true in practice). In our implementation, if nothing matches, we can simply return the first sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll want to lemmatize the context, so we've provided a helper function for you below that will convert a context to a list of its lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# NLTK uses a more expressive POS tag set than WordNet, which uses only four categories.\n",
    "# This function maps anything POS-tagged with NLTK to a POS tag that WordNet can recognize\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "    \n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2.8: Complete function `customized_lesk`\n",
    "\n",
    "`customized_lesk` takes the context (string) and the word, and returns the predicted sense. Remember, it considers the sense whose definition has the most common words with the context as the correct one. If all definitions have 0 words in common with the context, it returns the most frequent sense in our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e83f9638d8942f60fa0f99eb20c6b5e0",
     "grade": false,
     "grade_id": "cell-260f8d9281b39a9b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "params:\n",
    "  context: <string> the context of the given word in the sentence\n",
    "  word: <string> the given (ambiguous) word\n",
    "return:\n",
    "  pred_sense: <string> the correct sense produced by our self-made customized Lesk\n",
    "'''\n",
    "def customized_lesk(context, word, pos):\n",
    "    context_words = context.lower().split()\n",
    "    senses = wn.synsets(word, pos=pos)\n",
    "    max_overlap = 0\n",
    "    pred_sense = None\n",
    "    \n",
    "    for sense in senses:\n",
    "\n",
    "        # This is a definition of the sense as a string\n",
    "        definition = sense.definition()\n",
    "        # This is a list of example sentences \n",
    "        examples = sense.examples()\n",
    "\n",
    "        # TODO: check for the term-level overlap between the sentence context\n",
    "        # and current sense's *expanded gloss* which includes its definition and all\n",
    "        # the examples for the sense. In comparing, you'll want to lemmatize the\n",
    "        # content in the expanded gloss using the lemmatize_sentence() function \n",
    "        # and make sure everything is lower case to maximize the potential for \n",
    "        # overlap. Note that we're computing the overlap over the unique words.\n",
    "        #\n",
    "        # If the current sense has more overlap than the previous-best match\n",
    "        # update the previous max_overlap and pred_sense\n",
    "        \n",
    "        #HINT: Create a lesk context list \n",
    "        #HINT: Extend this list upon the lemmative_sentence function while passing the current definition as the parameter\n",
    "        #HINT: iterate through each item in examples:\n",
    "            #HINT: once again, extend the context list over lemmetize_sentence using the current team\n",
    "        #HINT: Iterate through each item in examples: \n",
    "            #HINT: once again, extend the context list over lemmative_sentences using the current item\n",
    "        \n",
    "        #HINT: The context list should only include lower case words\n",
    "        #Create a count variable equal to the number of commonalities of contect_words and the context list you created \n",
    "\n",
    "        #HINT: If our count is greater than max_overlap:\n",
    "            #HINT: Redefine max_overlap to our new maximum (count)\n",
    "            #HINT: Redefine pred_sense as our current sense \n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        lesk_context_list = []\n",
    "        lesk_context_list.extend(lemmatize_sentence(definition.lower()))\n",
    "        for item in examples:\n",
    "            lesk_context_list.extend(lemmatize_sentence(item.lower()))\n",
    "            \n",
    "        commonalities = len(list(set(context_words) & set(lesk_context_list)))\n",
    "        \n",
    "        if commonalities > max_overlap:\n",
    "            max_overlap = commonalities\n",
    "            pred_sense = sense\n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # If nothing matched, guess the most frequent sense, which is typically \n",
    "    # the first sense in the database (as designed in WordNet)\n",
    "    if max_overlap == 0:\n",
    "        pred_sense = senses[0]\n",
    "    \n",
    "    #print(pred_sense, max_overlap)\n",
    "    return pred_sense.name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.9: Complete the following code for evaluating Lesk WSD\n",
    "Note that this cell can take ~3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "def6a65b7bdb33c373f9e4b607002fc8",
     "grade": false,
     "grade_id": "cell-0bb447f10a8aade6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79/79 [00:46<00:00,  1.70it/s]\n"
     ]
    }
   ],
   "source": [
    "lesk_results = {}\n",
    "\n",
    "# import nltk\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "for word_pos in tqdm(test_data_dict):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "    # TODO - retrieve the data from 'test_data_dict' for X_test and y_test\n",
    "    # YOUR CODE HERE\n",
    "    X_test, y_test = test_data_dict[lemma_pos][0], test_data_dict[lemma_pos][1]\n",
    "\n",
    "    word, pos = word_pos.split('.')\n",
    "    \n",
    "    for i in range(len(X_test)):\n",
    "        ### TODO - uncomment and finish the following line\n",
    "        pred_sense = customized_lesk(str(i), word, pos)\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        \n",
    "        if pred_sense == y_test[i]:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    lesk_results[word] = correct / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison: Supervised Learning Approach VS. Knowledge Based Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df = pd.DataFrame() # columns=['lemma',  'LogReg score','Lesk score',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df['lemma'] = sorted(list(clf_results.keys()))\n",
    "comparison_df['LogReg score'] = [item[1] for item in sorted(clf_results.items(), key=lambda x: x[0])]\n",
    "comparison_df['Lesk score'] = [item[1] for item in sorted(lesk_results.items(), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.10: Print the accuracy score of supervised learning approach for word \"group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fc021383f1ae2aec414cabaebc05a639",
     "grade": true,
     "grade_id": "cell-3c3abd66c0e7e00a",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9888059701492538\n"
     ]
    }
   ],
   "source": [
    "print(comparison_df[comparison_df['lemma']=='group.n']['LogReg score'].values[0])\n",
    "# hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.11: Print the accuracy score of knowledge-based approach for word \"group\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6437e7a87a0571ae74c1397ffe2d6346",
     "grade": true,
     "grade_id": "cell-bb7b2b0a85796ab7",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(comparison_df[comparison_df['lemma']=='group.n']['Lesk score'].values[0])\n",
    "# hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.12: Print the accuracy score of supervised learning approach for word \"become\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d4bbdaf992380d2d8d04947ce9d527ba",
     "grade": true,
     "grade_id": "cell-96c6cdbaace169ac",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6376811594202898\n"
     ]
    }
   ],
   "source": [
    "print(comparison_df[comparison_df['lemma']=='become.v']['LogReg score'].values[0])\n",
    "# hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4.2.13: Print the accuracy score of knowledge-based approach for word \"become\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "98bb2b89704f2b311839c2c190296059",
     "grade": true,
     "grade_id": "cell-706c0c4f555ff853",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(comparison_df[comparison_df['lemma']=='become.v']['Lesk score'].values[0])\n",
    "# hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the whole comparison data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How correlated are the two scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_df['LogReg score'].corr(comparison_df['Lesk score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Logistic Regression accuracy: %f, mean Lesk accuracy: %f' %\n",
    "      (np.mean(comparison_df['LogReg score']), np.mean(comparison_df['Lesk score'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some more exploration \n",
    "\n",
    "Let's do some more advanced plotting with Seaborn. First, we'll convert everything into a long-form data frame for easier plotting. While we're at it, let's include the number of examples for each lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longform_df = defaultdict(list)\n",
    "for i, row in comparison_df.iterrows():\n",
    "    lemma_pos = row['lemma']\n",
    "    \n",
    "    lemma, pos = lemma_pos.split('.')\n",
    "    senses = wn.synsets(lemma, pos=pos)\n",
    "    \n",
    "    longform_df['lemma'].append(lemma_pos)\n",
    "    longform_df['Accuracy'].append(row['LogReg score'])\n",
    "    longform_df['Method'].append('Logistic Regression')\n",
    "    longform_df['lemma frequency'].append(lemma_counts[lemma_pos])\n",
    "    longform_df['num senses'].append(len(senses))\n",
    "    longform_df['lemma'].append(lemma_pos)\n",
    "    longform_df['Accuracy'].append(row['Lesk score'])\n",
    "    longform_df['Method'].append('Lesk')\n",
    "    longform_df['lemma frequency'].append(lemma_counts[lemma_pos])    \n",
    "    longform_df['num senses'].append(len(senses))    \n",
    "longform_df = pd.DataFrame(longform_df)\n",
    "longform_df['log lemma freq'] = np.log(longform_df['lemma frequency'])\n",
    "longform_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does do we do for each word? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=longform_df, x='lemma', y='Accuracy', hue='Method',\n",
    "               size='log lemma freq')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we sort words by their frequency? Does the ML-based approach do worse with fewer examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=longform_df[longform_df['log lemma freq'] < 8],  y='Accuracy', hue='Method',\n",
    "               x='log lemma freq', size='num senses')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try to make the relationship between accuracy and frequency more clear with a regression plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=longform_df[longform_df['log lemma freq'] < 8],  y='Accuracy', hue='Method',\n",
    "               x='log lemma freq')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about how the number of senses relates to accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=longform_df,  y='Accuracy', hue='Method', x='num senses')\n",
    "\n",
    "# Put the legend out of the figure\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Did you find anything interesting in comparison? What do you think is the reason(s) that contributes to your findings? Feel free to discuss and/or write your thoughts on slack.\n",
    "\n",
    "If this exercise has sparked your interest, here's a few ideas on what you could try next:\n",
    "* Try running on more rare senses in the data. When does it make sense to use Lesk versus a supervised learning approach?\n",
    "* Try a more sophisticated classifier. Could you get higher performance with a RandomForest?\n",
    "* In Exercise 2, you tried using dense representations, which often work well when we have short texts. It's a bit of work, but you could try loading in the googlenews vectors from Exercise do and comparing the dense representations of each context with the definition (or extended Lesk context!) to find the most similar. \n",
    "* If you're feeling very ambitions, try running on the SemEval WSD tasks, which have more data and data in languages other than English!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_applied_natural_language_processing_v1_assignment4_part2"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
