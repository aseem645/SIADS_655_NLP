{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2.1 - Train and Explore Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with word vectors\n",
    "What's the NBA of baseball? One way to answer this is to form an analogy: basketball is to NBA as baseball is to ... ? The answer is MLB (Major League Baseball). One of the more interested developments in the past decade for NLP has been new methods for learning word vectors that encode both the meaning of words _and_ relational knowledge to solve these kinds of analogy tests. In this notebook, you'll see how these work for yourself!\n",
    "\n",
    "The homework is broken up in several steps of preprocessing code (a constant fact of life for NLP), training a [word2vec](https://en.wikipedia.org/wiki/Word2vec) model using the [gensim](https://radimrehurek.com/gensim_3.8.3/models/word2vec.html) library and then exploring what the vectors have learned. For this notebook, you'll only train on a small set of the data to keep it quick (though this still learns a lot, as you'll see!); we've pre-computed a word2vec model for you on the full data as well, which you will use later in your evaluations. If you want to read up a bit more on word2vec, there's many great [blog](https://israelg99.github.io/2017-03-23-Word2Vec-Explained/) [posts](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) and you could check out the [original paper](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "This notebook will show you how to train your own vectors in the future too. Methods that learn distributional word vectors (like word2vec, LSA, or even counting) depend on the corpus they're trained on. Here, we're using our list of Wikipedia biographies. This corpus is quite rich in people, places, occupations, and all the things that people do. What all might a model learn from this? You'll see some of it in the tests we have prepared for you, but the model will know a lot more than what we've shown. Once you finish the core tasks, in your own exploration see what else the model has learned. Are there new types of analogies it encodes?\n",
    "\n",
    "As usual in NLP, there are many options and hyperparameters to choose for word2vec. We've chosen a few to show you here (e.g., tokenization options) to help you get started exploring the space. You can try training on more data, adjust the window size, or the minimum token frequency to see how these impact performance and what the model learns. If you discover something interesting, feel free to discuss!\n",
    "\n",
    "Finally, if you're feeling ambitious, try plotting pairs of words that have the same relationship, e.g., \"NBA\", \"basketball\", \"MLB\" and \"baseball\" and see if they have a geometric  relationship. As an example of this type of plot, see the plots for the word2vec alternative [glove](https://nlp.stanford.edu/projects/glove/) that also encodes this type of information. For plotting, try showing the first two principle components of the vectors (you'll first need to run PCA) or use [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). If you make any interesting plots or discover something cool, please feel free to share in Slack!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gzip\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from gensim.test.utils import datapath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 655"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce the burden on memory, we use only the first 10,000 biographies and save them into a list that we'll call `bios`. We've pre-saved them as a pickle object `bios.p` so that you can load it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bios = []\n",
    "with open('assets/bios.p', 'rb') as f:\n",
    "    bios = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use nltk `word_tokenize` to split the biographies in `bios` into words and each bio's words as a separate list into a list called `nltk_tokenized_bios`. Try wrapping these in a `tqdm` call to see how long it takes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "35dc1b130cf4f5a1f53ae8ed5b1a3572",
     "grade": false,
     "grade_id": "cell-ee0113aacf9b88ae",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72b177e9800c4eedb8da35b75f074b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "nltk_tokenized_bios = [word_tokenize(i) for i in tqdm(bios)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare NLTK's tokenization with a regular expression-based extraction. Write a regular expression to find all sequences of word characters (`\\w+`) for each biography in `bios` into words and each bio's words as a separate list into a list called `re_tokenized_bios`. Try wrapping these in a `tqdm` call to see how long it takes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "53df9e867c09cd34e7a5b20ae1a561d0",
     "grade": false,
     "grade_id": "cell-cee05145ae73b804",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe5d8816ef6d4710a3799fccd90f7e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "re_tokenized_bios = [re.findall(r\"\\w+\",i) for i in tqdm(bios)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A surprising speed difference! Let's count how many unique tokens we found from each. Use a `Counter` to count the number of unique words for each and call these `nltk_word_counts` and `re_word_counts` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4a2d6fcc9465c47294141832ac666c8c",
     "grade": false,
     "grade_id": "cell-686a39e7f44a07fb",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "nltk_word_counts = Counter(x for xs in nltk_tokenized_bios for x in set(xs))\n",
    "re_word_counts = Counter(x for xs in re_tokenized_bios for x in set(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(294549, 219293)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk_word_counts), len(re_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f734c0cb714144c8e5439ff8d4fb44f3",
     "grade": true,
     "grade_id": "cell-f2e29134fae0ac68",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what common words we missed through the imprecise regex matching. Create another `Counter` that contains the counts of all words not in `re_word_counts` but `nltk_word_counts` and call this `unique_word_counts`\n",
    "\n",
    "HINT: familiarize with how to use “Counter” from collections. \n",
    "Documentation: https://docs.python.org/3/library/collections.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7ab9c5657fe6cef1cbee0bacd460b8e9",
     "grade": false,
     "grade_id": "cell-b6e515d15e089fb7",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "unique_word_counts =  list({s for s in nltk_word_counts if s not in re_word_counts})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print what are the 20 most common NLTK-unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1a1ca244365dae1c5f95a97109e65a23",
     "grade": false,
     "grade_id": "cell-732f653a18074a41",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('7:15', 1),\n",
       " ('posts.', 1),\n",
       " ('166–167', 1),\n",
       " ('Wierusz-Kowalski', 1),\n",
       " ('====1975====', 1),\n",
       " ('Limpsfield-Oxted', 1),\n",
       " ('15–20', 1),\n",
       " (\"L'Action\", 1),\n",
       " ('KLAC-TV', 1),\n",
       " ('=', 1),\n",
       " ('four-movie', 1),\n",
       " ('non-monogamous', 1),\n",
       " ('b.2014', 1),\n",
       " ('Anti-Federalist', 1),\n",
       " ('34.1', 1),\n",
       " ('Forty-seven', 1),\n",
       " ('—one', 1),\n",
       " ('pre-development', 1),\n",
       " (\"L'important\", 1),\n",
       " ('Philemon—', 1)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "unique_word_counts = Counter(unique_word_counts)\n",
    "unique_word_counts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81881"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4ed86c14043912b56a4ed98a69889d81",
     "grade": true,
     "grade_id": "cell-93f75cf1d604e3e1",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting! Looks like we're mostly missing punctuation and a few abbreviations. In this case, we can probably get away with tokenizing with just a regex without missing too much. \n",
    "\n",
    "For simplicitly, let's treat the regex-based token as our final solution and label this as a list called `all_tokenized_bios` and call the `re_word_counts` as just `word_counts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokenized_bios = re_tokenized_bios \n",
    "word_counts = re_word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "219293"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de8065130e7fb8b592b5aee4f812bacb",
     "grade": true,
     "grade_id": "cell-a28af82a4d6f430c",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're dealing with lots of named entities, we didn't lower-case anything when tokenizing, which lets us potentially separate out \"Apple\" (the company) from \"apple\" (the fruit). Did we add many tokens from doing this? Create another `Counter` called `lowercase_word_counts` that records all the lower-case word counts and print the number of unique words. How many new upper-case words did we learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "9a3e5a7948e32964a16e955874aff7f2",
     "grade": false,
     "grade_id": "cell-480f8dc8276dd20a",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "lowercase_word_counts = [x.lower() for x in word_counts]\n",
    "lowercase_word_counts = Counter(lowercase_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(219293, 191199)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts), len(lowercase_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "76479312c3ef6b1d9ad195f82b6634c1",
     "grade": true,
     "grade_id": "cell-e8a63053f2aed85b",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.9 ms, sys: 130 µs, total: 40.1 ms\n",
      "Wall time: 39.4 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7791"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "len([w for w,c in word_counts.items() if c >= 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly train a word2vec model on the 10,000 tokenized biographies in `all_tokenized_bios`. This should train relatively quickly (~1 minute) and will let us do a few quick sanity tests. Call this model `quick_model` since it should be relative quick to train compared with training on the full set of biographies you have been using.\n",
    "\n",
    "Here, we'll use arguments to specify:\n",
    "* 50-dimensional vectors\n",
    "* a window size of +/-2\n",
    "* a minimum word frequency of 100\n",
    "* 4 threads to process in parallel\n",
    "* The class's `RANDOM_SEED`\n",
    "\n",
    "*Note:* in the gensim that is installed for this homework, the vector size is specified as `size` in the argument. (Some other versions of the library have different names for the argument!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "54451c8bef98f91d7ba8060e958dd7da",
     "grade": false,
     "grade_id": "cell-2a933727c8472b14",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 55s, sys: 2.49 s, total: 1min 58s\n",
      "Wall time: 35.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# YOUR CODE HERE\n",
    "quick_model = Word2Vec(sentences=all_tokenized_bios, min_count=100, size=50, window=2,  workers=4, seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word vectors are easily accessed through the `.wv` field of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_word_vectors = quick_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('physics', 0.8806302547454834),\n",
       " ('biology', 0.8794860243797302),\n",
       " ('psychology', 0.8791428208351135),\n",
       " ('mathematics', 0.8578429818153381),\n",
       " ('economics', 0.8430607914924622),\n",
       " ('medicine', 0.8429045081138611),\n",
       " ('astronomy', 0.8322444558143616),\n",
       " ('anthropology', 0.8304941058158875),\n",
       " ('sociology', 0.8193984627723694),\n",
       " ('engineering', 0.7988761067390442)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_word_vectors.similar_by_word(\"chemistry\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('baseball', 0.9110628962516785),\n",
       " ('basketball', 0.9079024195671082),\n",
       " ('hockey', 0.8721810579299927),\n",
       " ('soccer', 0.8670200109481812),\n",
       " ('tennis', 0.8545448780059814),\n",
       " ('golf', 0.8269231915473938),\n",
       " ('chess', 0.8118621110916138),\n",
       " ('cricket', 0.8074628114700317),\n",
       " ('wrestling', 0.7804355621337891),\n",
       " ('team', 0.7640689015388489)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quick_word_vectors.similar_by_word(\"football\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Word2Vec model trained on our Wikipedia corpus\n",
    "\n",
    "Training a model on the full data can take 20-60 minutes so we've precomputed a model for you at `assets/wikipedia.100.word-vecs.kv`. Let's load these vectors using `gensim.models.KeyedVectors.load` and call this `full_word_vectors`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ff743eba1cf4d23b102e2224da0277d5",
     "grade": false,
     "grade_id": "cell-704f746ce5bcfd31",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "full_word_vectors = gensim.models.KeyedVectors.load('assets/wikipedia.100.word-vecs.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a quick sanity check, print out the 10 most similar words for \"chemistry\" and \"football\" again using these new word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 0.8000362515449524),\n",
       " ('their', 0.6638365983963013),\n",
       " ('this', 0.6563833951950073),\n",
       " ('our', 0.6386459469795227),\n",
       " ('his', 0.62073814868927),\n",
       " ('The', 0.6133779883384705),\n",
       " ('a', 0.6116929054260254),\n",
       " ('another', 0.5941141247749329),\n",
       " ('her', 0.5800747275352478),\n",
       " ('every', 0.5692095160484314)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_vectors.similar_by_word(\"the\")[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('soccer', 0.8991492390632629),\n",
       " ('basketball', 0.8783006072044373),\n",
       " ('baseball', 0.8465266227722168),\n",
       " ('hockey', 0.8135026097297668),\n",
       " ('lacrosse', 0.7592754364013672),\n",
       " ('volleyball', 0.7534387111663818),\n",
       " ('rugby', 0.7447830438613892),\n",
       " ('softball', 0.7415613532066345),\n",
       " ('tennis', 0.7191575169563293),\n",
       " ('futsal', 0.7188178300857544)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_word_vectors.similar_by_word(\"football\")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity and Relatedness\n",
    "\n",
    "Looks like both models are capturing a surprising amount of information. Let's test how well both our quickly-learned and fully-trained vectors reflect human judgments of similarity and relatedness. Here, we'll use the [SimLex-999](https://fh295.github.io/simlex.html) and [WordSim-353](http://alfonseca.org/eng/research/wordsim353.html) benchmarks. The WordSim-353 dataset is notable for including judgments of both similarity and relatedness. \n",
    "\n",
    "To test on these datasets, we'll use the `evaluate_word_pairs` function of the `KeyedVectors` class which knows how to read these files and score them using both Pearson and Spearman's correlations. The datasets are stored at:\n",
    "* `assets/wordsim_similarity_goldstandard.txt`\n",
    "* `assets/wordsim_relatedness_goldstandard.txt`\n",
    "* `assets/SimLex-999.tsv`\n",
    "\n",
    "To start, let's compute similarities for `full_word_vectors` and `quick_word_vectors` on both SimLex999 and WordSim353. Save the results of each as variables named with the prefix and dataset, e.g., `full_simlex999` or `quick_wordsim353`, and print the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d5caac7e788e9990bf4867cb2c4df499",
     "grade": false,
     "grade_id": "cell-7c1a8ce54e6741da",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "full_simlex999 = full_word_vectors.evaluate_word_pairs(pairs='assets/SimLex-999.tsv')\n",
    "full_wordsim353 = full_word_vectors.evaluate_word_pairs(pairs='assets/wordsim_similarity_goldstandard.txt')\n",
    "\n",
    "quick_simlex999 =  quick_word_vectors.evaluate_word_pairs(pairs='assets/SimLex-999.tsv')\n",
    "quick_wordsim353 = quick_word_vectors.evaluate_word_pairs(pairs='assets/wordsim_similarity_goldstandard.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6e9bfa4262e2ec62fa99fdc42c9fb186",
     "grade": true,
     "grade_id": "cell-c0d54175013049e3",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for relatedness! Here, call your variables `full_relatedness353` and `quick_relatedness353`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1690e0313056a98490d52c3c32c8a4f0",
     "grade": false,
     "grade_id": "cell-f170023fb323b31d",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "full_relatedness353 = full_word_vectors.evaluate_word_pairs(pairs='assets/wordsim_relatedness_goldstandard.txt')\n",
    "quick_relatedness353 = quick_word_vectors.evaluate_word_pairs(pairs='assets/wordsim_relatedness_goldstandard.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0ae9c0d3658862f064ed40e447e4c83b",
     "grade": true,
     "grade_id": "cell-9737ff7a8819df14",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Analogies\n",
    "Word vectors can not only capture similarity in meaning but relational structure as well. Here, we'll look for analogies of the for `a:b::c:d` or \"a is to b as c is to d\". In solving these types of analogies, we'll use the `most_similar` function of `KeyedVectors` that will do the vector arithmetic, as described in the original word2vec  paper.\n",
    "\n",
    "In your first task, write a function `get_analogy` that takes arguments `a`, `b`, `c`, and uses the `most_similar` function to find and return the `d` item that is analogous to `b` in the `full_word_vectors` space.\n",
    "\n",
    "*NOTE:* you should make sure to pass lists to `most_similar`, not tuples (the latter will work but give incorrect results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "1abd6b36b0543f4136d3d0c5d650c7ef",
     "grade": false,
     "grade_id": "cell-3435c4d7f9f6ad9b",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "def get_analogy(a,b,c):\n",
    "    d = full_word_vectors.most_similar(positive=[b, c], negative=[a])\n",
    "    return d[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task: Find the appropriate analogy entities/concepts for the following relations\n",
    "\n",
    "For all questions below, please return your answer as a single `str`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation 1: Country::Capital"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the capital of each following countries, according to the fact that London is the capital of UK:\n",
    "1. France\n",
    "2. Germany\n",
    "3. Italy\n",
    "4. Austria\n",
    "5. Denmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "cf81bdb8dcc27363cd3553473c2ca11f",
     "grade": true,
     "grade_id": "cell-e8ef37e837980a39",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('UK', 'London', 'France')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3c49ed9316e3a61b9138774dff108b7c",
     "grade": true,
     "grade_id": "cell-8b009ff1e6303bcd",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('UK', 'London', 'Italy')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "69b30bf456990e4c41eb54f20830e7c2",
     "grade": true,
     "grade_id": "cell-2499aa9e12b46802",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berlin\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('UK', 'London', 'Germany')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "df426a8b5537b3b45044cf140a295bfd",
     "grade": true,
     "grade_id": "cell-33ca779e0942a548",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vienna\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('UK', 'London', 'Austria')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "57d99c4cb2d0d2c564dba7420acc9400",
     "grade": true,
     "grade_id": "cell-ce92e4b932a6db8d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copenhagen\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('UK', 'London', 'Denmark')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, pretty amazing! Considering that these are only trained on Wikipedia biographies, the model has captured quite a bit of information about places."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation 2: Association::Sports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of athletes in our dataset. Let's see if we can recover the relationship between sports associations and the sports played in them, NBA is to basketball as NHL is to ???\n",
    "1. NFL - National ????? League\n",
    "2. NHL - National ????? League\n",
    "3. MLB - Major League ?????\n",
    "4. MLS - Major League ?????\n",
    "5. NLL - National ????? League"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "de1e5e17ed0003b394e6bd0ba7e4cf7d",
     "grade": true,
     "grade_id": "cell-cb1361d870f81739",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NBA', 'basketball', 'NHL')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8663a106141d626cb0038ea0078bd4b0",
     "grade": true,
     "grade_id": "cell-45df4b62b80d77b9",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NBA', 'basketball', 'MLB')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "be373b6b5295fbc6ae532ad89d919421",
     "grade": true,
     "grade_id": "cell-bacdf8ec027f4c3d",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NBA', 'basketball', 'MLS')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "00163a1df6fd66d3e13a86431aef8d8d",
     "grade": true,
     "grade_id": "cell-6cc5455d282c50ad",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseball\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NBA', 'basketball', 'NFL')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3fc119fed407ee4b60b4fb1b9c03ee1b",
     "grade": true,
     "grade_id": "cell-218d681fe8f69956",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NBA', 'basketball', 'NLL')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try a different one to predict for the NFL?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fdacdd56c1db8567a1738b840b184437",
     "grade": true,
     "grade_id": "cell-581a2690c8ef167e",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basketball\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('MLB', 'baseball', 'NFL')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we try the reverse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97b9c6555fde8d58471b8b4e05407252",
     "grade": true,
     "grade_id": "cell-028070e83f343132",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soccer\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('NFL', 'football', 'NBA')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh! This suggests that NFL in our dataset is occurring in different contexts than other sports leagues. But it can be tough to tell what's fully responsible! Perhaps the use of football as both \"soccer\" and \"gridiron football\" causes an issue? The model sure seems to like \"soccer\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the NFL Kickoff Live\n",
      "NBA and NFL teams inspiring\n",
      "two future NFL Hall of\n",
      "of the NFL In 1977\n",
      "in the NFL There were\n",
      "to an NFL franchise but\n",
      "let go NFL career Philadelphia\n",
      "on the NFL had changed\n",
      "by the NFL that a\n",
      "of the NFL championship game\n",
      "lowest in NFL history to\n",
      "of the NFL rosters had\n",
      "displace the NFL s sovereignty\n",
      "its history NFL commissioner 1946\n",
      "and the NFL schedule 1946\n",
      "the first NFL commissioner in\n",
      "among the NFL owners since\n",
      "the next NFL owners meeting\n",
      "ban any NFL associated personnel\n",
      "of the NFL to investigate\n",
      "scams AAFC NFL merger 1948\n",
      "1950 The NFL s struggle\n",
      "prevented the NFL for showing\n",
      "actualized the NFL s first\n",
      "an AAFC NFL merger was\n",
      "of the NFL 1950 1956\n",
      "into the NFL which forbid\n",
      "for the NFL He negotiated\n",
      "of the NFL s rise\n",
      "declared the NFL was subject\n",
      "that the NFL should be\n",
      "the first NFL championship game\n",
      "named interim NFL commissioner for\n",
      "of the NFL franchises were\n",
      "regional CBS NFL and CBS\n",
      "shows for NFL broadcasts and\n",
      "returned to NFL studio hosting\n",
      "last hosted NFL telecasts for\n",
      "for The NFL on NBC\n",
      "play of NFL games with\n",
      "NBC s NFL coverage the\n",
      "or the NFL season because\n",
      "Inside the NFL Costas remained\n",
      "Inside the NFL through the\n",
      "the 2007 NFL season He\n",
      "and former NFL legends Dan\n",
      "during the NFL season Costas\n",
      "being loaded NFL Network As\n",
      "NBC and NFL Network in\n",
      "1976 1979 NFL on CBS\n",
      "1980 1983 NFL on NBC\n",
      "2006 2016 NFL on NBC\n",
      "Inside the NFL Host 2003\n",
      "2016 NBC NFL Network Host\n",
      "previous owners NFL player Tom\n",
      "described the NFL anthem protests\n",
      "Beckham and NFL running back\n",
      "DirecTV s NFL Sunday Ticket\n",
      "Football League NFL players The\n",
      "the 2006 NFL season In\n",
      "1972 73 NFL 6 5\n",
      "with former NFL player Rocky\n"
     ]
    }
   ],
   "source": [
    "for bio in all_tokenized_bios[:1000]:\n",
    "    for i, w in enumerate(bio):\n",
    "        if w == 'NFL':\n",
    "            print(' '.join(bio[i-2:i+3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation 3: Expert::Field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find what fields do the following experts have expertise in, according to the fact that biologists have expertise in biology:\n",
    "1. sociologists\n",
    "2. psychologists\n",
    "3. neuroscientists\n",
    "4. zoologists\n",
    "5. physiologists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0c55f8b3c82230f1351f340d888aabdb",
     "grade": true,
     "grade_id": "cell-7ab25f88ce2c9391",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chemistry\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('biologists', 'biology', 'chemists')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "73ec390b68793bc0372b807571ff0ecc",
     "grade": true,
     "grade_id": "cell-348019114f528f06",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oncology\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('biologists', 'biology', 'oncologist')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "23ce219a927a2d328434f78518bad8ab",
     "grade": true,
     "grade_id": "cell-03094750a8e101ba",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "art\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('biologist', 'biology', 'artist')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "369c33fe14b10d0894272ad6aa57107a",
     "grade": true,
     "grade_id": "cell-582bfc5a3ebb1afd",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "botany\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('biologist', 'biology', 'zoologist')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "62062e373f82f295d8a23d27713a706d",
     "grade": true,
     "grade_id": "cell-9d02637f476a9e80",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "archaeology\n"
     ]
    }
   ],
   "source": [
    "answer = get_analogy('biologists', 'biology', 'archaeologists')\n",
    "print(answer)\n",
    "#hidden tests are within this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad! It seems to have learned something about what the fields and people do!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "\n",
    "Which tasks surprised you more? These results show that our model knows a surprising amount. What else does it know? Feel free to try out new types of relationships and words. Keep in mind that it only knows about information in relationship to people since we've trained it on biographies. \n",
    "\n",
    "One thing we didn't try was training our model with multi-word expressions (MWEs). Gensim actually supports finding these for us using its `Phrases` package when processing the corpus and learning word vectors. This would let us compare things like `United States` as a phrase. Which kinds of MWEs would be fun to test out? Feel free to retrain the model here and test out new analogies on MWEs; if you find interest ones, post on course Slack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "mads_applied_natural_language_processing_v1_assignment2_part1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
